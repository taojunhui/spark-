{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fa11991",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bf9b73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "#创建一个配置对象\n",
    "conf = SparkConf().setMaster(\"spark://192.168.206.101:7077\").setAppName(\"KMeans\")\n",
    "conf.setAll([('spark.driver.memory','700m'), ('spark.executor.memory', '3000m'),('spark.executor.cores', 2)])\n",
    "#创建一个SparkContext对象\n",
    "sc = SparkContext(conf=conf)\n",
    "#创建sparksession\n",
    "spark = SparkSession.builder.master('spark://192.168.206.101:7077').getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80b39b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取数据，创建DataFrame\n",
    "dataWithoutHeader = spark.read.option(\"inferSchema\",True).option(\"header\", False).csv(\"hdfs:///user/ds/kddcup.data_10_percent_corrected\")\n",
    "data = dataWithoutHeader.toDF(\"duration\", \"protocol_type\", \"service\", \"flag\",\n",
    "\"src_bytes\", \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\",\n",
    "\"hot\", \"num_failed_logins\", \"logged_in\", \"num_compromised\",\n",
    "\"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\",\n",
    "\"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\n",
    "\"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\",\n",
    "\"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\",\n",
    "\"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\",\n",
    "\"dst_host_count\", \"dst_host_srv_count\",\n",
    "\"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\",\n",
    "\"dst_host_same_src_port_rate\", \"dst_host_srv_diff_host_rate\",\n",
    "\"dst_host_serror_rate\", \"dst_host_srv_serror_rate\",\n",
    "\"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\",\n",
    "\"label\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be499f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+\n",
      "|           label| count|\n",
      "+----------------+------+\n",
      "|          smurf.|280790|\n",
      "|        neptune.|107201|\n",
      "|         normal.| 97278|\n",
      "|           back.|  2203|\n",
      "|          satan.|  1589|\n",
      "|        ipsweep.|  1247|\n",
      "|      portsweep.|  1040|\n",
      "|    warezclient.|  1020|\n",
      "|       teardrop.|   979|\n",
      "|            pod.|   264|\n",
      "|           nmap.|   231|\n",
      "|   guess_passwd.|    53|\n",
      "|buffer_overflow.|    30|\n",
      "|           land.|    21|\n",
      "|    warezmaster.|    20|\n",
      "|           imap.|    12|\n",
      "|        rootkit.|    10|\n",
      "|     loadmodule.|     9|\n",
      "|      ftp_write.|     8|\n",
      "|       multihop.|     7|\n",
      "|            phf.|     4|\n",
      "|           perl.|     3|\n",
      "|            spy.|     2|\n",
      "+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#查看数据，根据类别标号，分类统计样本个数，然后降序\n",
    "data.select(\"label\").groupBy(\"label\").count().orderBy(\"count\", ascending=False).show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0767b5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans, KMeansModel\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "numericOnly = data.drop(\"protocol_type\", \"service\", \"flag\").cache()\n",
    "assembler = VectorAssembler().setInputCols([column for column in numericOnly.columns if column!='label']).setOutputCol(\"featureVector\")\n",
    "kmeans = KMeans().\\\n",
    "setPredictionCol(\"cluster\").\\\n",
    "setFeaturesCol(\"featureVector\")\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline().setStages([assembler, kmeans])\n",
    "pipelineModel = pipeline.fit(numericOnly)\n",
    "kmeansModel = pipelineModel.stages[-1]\n",
    "kmeansModel.clusterCenters\n",
    "for center in kmeansModel.clusterCenters():\n",
    "\tprint(center)\n",
    "    \n",
    "withCluster = pipelineModel.transform(numericOnly)\n",
    "withCluster.select(\"cluster\",\"label\").groupBy('cluster', 'label').count().orderBy(['cluster', 'label'], ascending=[1,0]).show(25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6fde99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def clusteringScore0(data, k):\n",
    "    #创建一特征向量\n",
    "    assembler = VectorAssembler().setInputCols([e for e in data.columns if e!='label']).setOutputCol('featureVector')\n",
    "    #创建kmeans\n",
    "    kmeans = KMeans().setSeed(random.randint(1, 2**63-1)).setK(k).setPredictionCol(\"cluster\").setFeaturesCol(\"featureVector\")\n",
    "    #创建一个管道\n",
    "    pipeline = Pipeline().setStages([assembler, kmeans])\n",
    "    kmeansModel = pipeline.fit(data).stages[-1]\n",
    "    return kmeansModel.computeCost(assembler.transform(data))/data.count()\n",
    "\n",
    "for k in range(20, 101,20):\n",
    "    print(k,clusteringScore0(numericOnly, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0829c337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusteringScore1(data, k):\n",
    "    #创建一特征向量\n",
    "    assembler = VectorAssembler().setInputCols([e for e in data.columns if e!='label']).setOutputCol('featureVector')\n",
    "    #创建kmeans\n",
    "    #设置最大迭代次数。默认20， 设置阈值，该阈值控制聚类过程中簇质点进行有效移动的最小值。\n",
    "    kmeans = KMeans().\\\n",
    "    setSeed(random.randint(1, 2**63-1)).\\\n",
    "    setK(k).\\\n",
    "    setPredictionCol(\"cluster\").\\\n",
    "    setFeaturesCol(\"featureVector\").\\\n",
    "    setMaxIter(70).\\\n",
    "    setTol(1.0e-7)\n",
    "    #创建一个管道\n",
    "    pipeline = Pipeline().setStages([assembler, kmeans])\n",
    "    kmeansModel = pipeline.fit(data).stages[-1]\n",
    "    return kmeansModel.computeCost(assembler.transform(data))/data.count()\n",
    "\n",
    "for k in range(20, 201,20):\n",
    "    print(k,clusteringScore1(numericOnly, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26737afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#特征的规范化\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "def clusteringScore2(data, k):\n",
    "    assembler = VectorAssembler().setInputCols([e for e in data.columns if e!='label']).setOutputCol('featureVector')\n",
    "    #创建一个StandardScaler用于特征规范化\n",
    "    scaler = StandardScaler().setInputCol(\"featureVector\").setOutputCol(\"scaledFeatureVector\").setWithStd(True).setWithMean(False)\n",
    "    kmeans = KMeans().\\\n",
    "    setSeed(random.randint(1, 2**63-1)).\\\n",
    "    setK(k).\\\n",
    "    setPredictionCol(\"cluster\").\\\n",
    "    setFeaturesCol(\"scaledFeatureVector\").\\\n",
    "    setMaxIter(40).\\\n",
    "    setTol(1.0e-5)\n",
    "    pipeline = Pipeline().setStages([assembler, scaler, kmeans])\n",
    "    pipelineModel = pipeline.fit(data)\n",
    "    kmeansModel = pipeline.fit(data).stages[-1]\n",
    "    return kmeansModel.computeCost(pipelineModel.transform(data))/data.count()\n",
    "\n",
    "for k in range(60, 400, 30):\n",
    "    print((k,clusteringScore2(numericOnly, k)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3de167",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#类别型变量\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "def oneHotPipeline(inputCol):\n",
    "    \"\"\"类别型变量\"\"\"\n",
    "    #使用StringIndexer将字符串转化为整数索引\n",
    "    indexer = StringIndexer().setInputCol(inputCol).setOutputCol(inputCol+\"_indexed\")\n",
    "    #编码成一个向量\n",
    "    encoder = OneHotEncoder().setInputCol(inputCol+\"_indexed\").setOutputCol(inputCol+\"_vec\")\n",
    "    pipeline = Pipeline().setStages((indexer, encoder))\n",
    "    return pipeline, inputCol+\"_vec\"\n",
    "\n",
    "def clusteringScore3(data, k):\n",
    "    protoTypeEncoder,protoTypeVecCol = oneHotPipeline(\"protocol_type\")\n",
    "    serviceEncoder,serviceVecCol = oneHotPipeline(\"service\")\n",
    "    flagEncoder,flagVecCol = oneHotPipeline(\"flag\" )\n",
    "    assemblerCols = set(data.columns) - {\"label\", \"protocol_type\", \"service\", \"flag\"}\n",
    "    assemblerCols.update([protoTypeVecCol, serviceVecCol, flagVecCol])\n",
    "    assembler = VectorAssembler().setInputCols(list(assemblerCols)).setOutputCol(\"featureVector\")\n",
    "    scaler = StandardScaler().\\\n",
    "    setInputCol(\"featureVector\").\\\n",
    "    setOutputCol(\"scaledFeatureVector\").\\\n",
    "    setWithStd(True).\\\n",
    "    setWithMean(False)\n",
    "    \n",
    "    kmeans = KMeans().\\\n",
    "    setSeed(random.randint(1, 2**63-1)).\\\n",
    "    setK(k).\\\n",
    "    setPredictionCol(\"cluster\").\\\n",
    "    setFeaturesCol(\"scaledFeatureVector\").\\\n",
    "    setMaxIter(40).\\\n",
    "    setTol(1.0e-5)\n",
    "    \n",
    "    pipeline = Pipeline().setStages([protoTypeEncoder, serviceEncoder, flagEncoder, assembler, scaler, kmeans])\n",
    "    pipelineModel = pipeline.fit(data)\n",
    "    kmeansModel = pipeline.fit(data).stages[-1]\n",
    "    #返回每个聚簇\n",
    "    return kmeansModel.computeCost(pipelineModel.transform(data))/data.count()\n",
    "\n",
    "for k in range(60, 400, 30):\n",
    "    print((k, clusteringScore3(data, k))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06658273",
   "metadata": {},
   "outputs": [],
   "source": [
    "#利用标号的熵信息\n",
    "import math\n",
    "\n",
    "def fitPipeline4(data, k):\n",
    "    protoTypeEncoder,protoTypeVecCol = oneHotPipeline(\"protocol_type\")\n",
    "    serviceEncoder,serviceVecCol = oneHotPipeline(\"service\")\n",
    "    flagEncoder,flagVecCol = oneHotPipeline(\"flag\" )\n",
    "    assemblerCols = set(data.columns) - {\"label\", \"protocol_type\", \"service\", \"flag\"}\n",
    "    assemblerCols.update([protoTypeVecCol, serviceVecCol, flagVecCol])\n",
    "    print(assemblerCols)\n",
    "    assembler = VectorAssembler().setInputCols(list(assemblerCols)).setOutputCol(\"featureVector\")\n",
    "    scaler = StandardScaler().\\\n",
    "    setInputCol(\"featureVector\").\\\n",
    "    setOutputCol(\"scaledFeatureVector\").\\\n",
    "    setWithStd(True).\\\n",
    "    setWithMean(False)\n",
    "    \n",
    "    kmeans = KMeans().\\\n",
    "    setSeed(random.randint(1, 2**63-1)).\\\n",
    "    setK(k).\\\n",
    "    setPredictionCol(\"cluster\").\\\n",
    "    setFeaturesCol(\"scaledFeatureVector\").\\\n",
    "    setMaxIter(40).\\\n",
    "    setTol(1.0e-5)\n",
    "    \n",
    "    pipeline = Pipeline().setStages([protoTypeEncoder, serviceEncoder, flagEncoder, assembler, scaler, kmeans])\n",
    "    return pipeline.fit(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88edaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(counts):\n",
    "    \"\"\"熵函数\"\"\"\n",
    "    def map_f(v):\n",
    "        p= v/n\n",
    "        return -p*math.log(p)\n",
    "\n",
    "    values = counts.filter(lambda a: a > 0)\n",
    "    n = values.map(lambda e: float(e)).sum()\n",
    "    return values.map(mapf).sum()\n",
    "    \n",
    "def clusteringScore4(data, k):\n",
    "    pipelineModel = fitPipeline4(data, k)\n",
    "    \n",
    "    #预测每一个聚类\n",
    "    clusterLabel = pipelineModel.transform(data).select('cluster','label')\n",
    "    clusterLabel.show()\n",
    "    weightedClusterEntropy = clusterLabel.groupBy('cluster')\n",
    "    \n",
    "clusteringScore4(data, 180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f584f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors, Vector\n",
    "\n",
    "\n",
    "def buildAnomalyDetector(data):\n",
    "   \n",
    "    #训练模型\n",
    "    pipelineModel = fitPipeline4(data, 240)\n",
    "    #查看数据， 统计每个簇中个标签的数量\n",
    "    countByClusterLabel = pipelineModel.transform(data).select(\"cluster\",\"label\").groupby(\"cluster\",\"label\").count().orderBy(\"cluster\",\"label\")\n",
    "    countByClusterLabel.show()\n",
    "    #获取质点， 聚类中心\n",
    "    kMeansModel = pipelineModel.stages[-1]\n",
    "    \n",
    "    centroids = kMeansModel.clusterCenters()\n",
    "    #转化数据\n",
    "    clustered = pipelineModel.transform(data)\n",
    "    #获取阈值\n",
    "    threshold = clustered.select(\"cluster\", \"scaledFeatureVector\").\\\n",
    "    rdd.\\\n",
    "    map(lambda e: Vectors.squared_distance(centroids[e[0]],e[1])).\\\n",
    "    sortBy(lambda e:e,ascending=False).\\\n",
    "    take(100)[-1]\n",
    "    originalCols = data.columns\n",
    "    #获取已知的异常列表\n",
    "    known = data.select(\"label\").distinct().collect()\n",
    "    known = [row[0] for row in known]\n",
    "    print(known)\n",
    "    #读取新数据\n",
    "    dataWithoutHeader = spark.read.option(\"inferSchema\",True).option(\"header\", False).csv(\"hdfs:///user/ds/corrected\")\n",
    "    new_data = dataWithoutHeader.toDF(\"duration\", \"protocol_type\", \"service\", \"flag\",\n",
    "    \"src_bytes\", \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\",\n",
    "    \"hot\", \"num_failed_logins\", \"logged_in\", \"num_compromised\",\n",
    "    \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\",\n",
    "    \"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\n",
    "    \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\",\n",
    "    \"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\",\n",
    "    \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\",\n",
    "    \"dst_host_count\", \"dst_host_srv_count\",\n",
    "    \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\",\n",
    "    \"dst_host_same_src_port_rate\", \"dst_host_srv_diff_host_rate\",\n",
    "    \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\",\n",
    "    \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\",\n",
    "    \"label\")\n",
    "    \n",
    "    #转化新的数据\n",
    "    print(\"@@@@@@@@@@@@@@@@@@@\")\n",
    "    #result = pipelineModel.transform(new_data)\n",
    "    #result.printSchema()\n",
    "    #result.show()\n",
    "    #correct_result = result.\\\n",
    "    #(\"cluster\", \"scalselectedFeatureVector\",\"label\").\\\n",
    "    #rdd.\\\n",
    "    #filter(lambda row:(Vectors.squared_distance(centroids[row[0]],row[1]) >= threshold and row[2] not in known) or (Vectors.squared_distance(centroids[row[0]],row[1]) < threshold and row[2] in known) )\n",
    "    print(\"@@@@@@@@@@@@@@@@@@@\")\n",
    "    pipeline = Pipeline().setStages( pipelineModel.stages[:-1])\n",
    "    pipelineModel = pipeline.fit(new_data)\n",
    "    new_processed_data=pipelineModel.transform(new_data)\n",
    "    \n",
    "    def isCorrect(row):\n",
    "        min_dis = None\n",
    "        #计算最到各个质点小距离\n",
    "        for center in centroids:\n",
    "            dis =  Vectors.squared_distance(center,row[0])\n",
    "            if min_dis is None:\n",
    "                min_dis = dis\n",
    "            min_dis = min(min_dis,dis)\n",
    "            \n",
    "        return (min_dis >= threshold and row[0] not in known) or (min_dis < threshold and row[0] in known)\n",
    "    #筛选出预测正确的结果\n",
    "    correct_result = new_processed_data.select(\"scaledFeatureVector\",\"label\").rdd.filter(isCorrect)\n",
    "    #输出数据\n",
    "    correct_result.foreach(print)\n",
    "    return correct_result.count()/ new_processed_data.count()\n",
    "\n",
    "accuracy = buildAnomalyDetector(data)  \n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fd232f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788282a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf,PandasUDFType\n",
    "\n",
    "df3 = spark.createDataFrame(\n",
    "\n",
    "[(\"a\", 1, 0), (\"a\", -1, 42), (\"b\", 3, -1), (\"b\", 10, -2)],\n",
    "\n",
    "(\"key\", \"value1\", \"value2\")\n",
    "\n",
    ")\n",
    "\n",
    "df3.show()\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "schema = StructType([\n",
    "\n",
    "    StructField(\"key\", StringType()),\n",
    "\n",
    "    StructField(\"avg_value1\", DoubleType()),\n",
    "\n",
    "    StructField(\"avg_value2\", DoubleType()),\n",
    "\n",
    "    StructField(\"sum_avg\", DoubleType()),\n",
    "\n",
    "    StructField(\"sub_avg\", DoubleType())\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "@pandas_udf(schema, functionType=PandasUDFType.GROUPED_MAP)\n",
    "def g(df):\n",
    "    gr = df['key'].iloc[0]\n",
    "    x = df.value1.mean()\n",
    "    y = df.value2.mean()\n",
    "    w = df.value1.mean() + df.value2.mean()\n",
    "    z = df.value1.mean() - df.value2.mean()\n",
    "    return pd.DataFrame([[gr]+[x]+[y]+[w]+[z]])\n",
    "\n",
    "df3.groupby(\"key\").apply(g).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862b9ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试部分\n",
    "pipelineModel = fitPipeline4(data, 240)\n",
    "countByClusterLabel = pipelineModel.transform(data).select(\"cluster\",\"label\").groupby(\"cluster\",\"label\").count().orderBy(\"cluster\",\"label\")\n",
    "countByClusterLabel.show()\n",
    "kMeansModel = pipelineModel.stages[-1]\n",
    "    #质点， 聚类中心\n",
    "centroids = kMeansModel.clusterCenters()\n",
    "    #转化数据\n",
    "clustered = pipelineModel.transform(data)\n",
    "    #获取阈值\n",
    "threshold = clustered.select(\"cluster\", \"scaledFeatureVector\").\\\n",
    "    rdd.\\\n",
    "    map(lambda e: Vectors.squared_distance(centroids[e[0]],e[1])).\\\n",
    "    sortBy(lambda e:e,ascending=False).\\\n",
    "    take(100)[-1]\n",
    "    \n",
    "    \n",
    "L100=clustered.select(\"cluster\", \"scaledFeatureVector\").\\\n",
    "    rdd.\\\n",
    "    map(lambda e: Vectors.squared_distance(centroids[e[0]],e[1])).\\\n",
    "    take(100)\n",
    "    \n",
    "print(L100)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
